{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADA - Project Milestone 2: \n",
    "# *Analysis of speech behaviours between genders*\n",
    "\n",
    "## Context\n",
    "\n",
    "In this project, we are going to analyze data from Quotebank. Quotebank, as the name suggests, is an open corpus which gathers 178 million quotations from 2008 and 2020. Still, in this project, we will only focus on the most recent quotations, being from 2015 to 2020, accounting from a little over a million quotations.\n",
    "\n",
    "We are interested in using this dataset to answer the following question: Do speech behaviours related to confidence and uncertainty vary between men and women?\n",
    "\n",
    "To answer this question, we'll go through the following points:\n",
    "\n",
    "1. To what extent can we observe the differences in communicative acts in relation to gender within a professional area? Are there noticeable differences between those professional areas?\n",
    "3. What are the roles of nationality, culture/tradition (religion, ethnic groups), and education (whether the speaker obtained an academic degree) in determining those differences in speech between men and women? How are the lines drawn between the language we use and the environment around us?\n",
    "4. Has there been a possible change over time (from 2015 to 2020)?\n",
    "\n",
    "In order to have access to the speakers' information (e.g., speakers' genders), we use the open source data from wikidata (https://www.wikidata.org/wiki/Wikidata:Main_Page).\n",
    "\n",
    "To analyse speech uncertainty, we adapted the public uncertainty detection classifier from the following paper \"P. A. Jean, S. Harispe, S. Ranwez, P. Bellot, and J. Montmain, “[Uncertainty detection in natural language: A probabilistic model](https://www.researchgate.net/publication/303842922)” ACM Int. Conf. Proceeding Ser., vol. 13-15-June, no. June, 2016, doi: 10.1145/2912845.2912873\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Table of contents\n",
    "\n",
    "[1. Pre-processing of the data](#pre-processing) \n",
    "- [Imports](#1imports)\n",
    "- [Pathways](#1pathways)\n",
    "- [Functions](#1functions)\n",
    "- [Merging files from wikidata into one file containing the 9 million speakers](#1merging)\n",
    "- [1.1 Loading and pre-processing of Quotebank data](#1.1)\n",
    "- [1.2 Loading wikidata labels](#1.2) \n",
    "- [1.3 Pre-processing of wikidata](#1.3)\n",
    "- [1.4 Exploratory Data Analysis of wikidata](#1.4)\n",
    "\n",
    "[2. Creation of the lists of similar professions](#professions)\n",
    "\n",
    "[3. Classification of the quotes](#classifier)\n",
    "\n",
    "[4. Statistical-analysis](#analysis)\n",
    "\n",
    "ADD POINT 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pre-processing of the data <a name=\"pre-processing\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports <a name=\"1imports\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by importing our libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pathways <a name=\"1pathways\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to run this project, you need to download the directories and files from the following drive: https://drive.google.com/drive/folders/1UgvnLUFhs14NDcZYH6NuZx2f_YC5i06N?usp=sharing. The data folders \"Data\" and \"Data_parquet\" need to be added in the same directory as this notebook. The two other directories (\"Classifier\" and \"txt_files\") will be needed in the [Classification of the quotes](#classifier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_PATH contains all the quotes as well as the wikidata labels' descriptions\n",
    "DATA_PATH = 'Data/'\n",
    "# PATH_PARQUET contains all the data from wikidata\n",
    "PATH_PARQUET = 'Data_parquet/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions <a name=\"1functions\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions are needed for the pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saving_wikidata(path):\n",
    "    \"\"\"\n",
    "    Transforms all the wikidata files in one dataset saved as pickle. \n",
    "    This allows to load the file and use it more quickly and easily.\n",
    "    Inputs:\n",
    "        * path : pathway where to save the pickle file\n",
    "    \"\"\"\n",
    "    wikidata_all = pd.DataFrame()\n",
    "    for i in range(1,16):\n",
    "        if i < 10:\n",
    "            DATA_FILE = 'part-0000{}-0d587965-3d8f-41ce-9771-5b8c9024dce9-c000.snappy.parquet'.format(i)\n",
    "        else:\n",
    "            DATA_FILE = 'part-000{}-0d587965-3d8f-41ce-9771-5b8c9024dce9-c000.snappy.parquet'.format(i)\n",
    "        wikidata = pd.read_parquet(PATH_PARQUET + DATA_FILE)\n",
    "        wikidata_all = pd.concat([wikidata_all, wikidata])\n",
    "    wikidata_all.to_pickle(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Merging files from wikidata into one file containing the 9 million speakers <a name=\"1merging\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we create the file \"all_wikidata.pck\" containing all the raw data from wikidata. We already ran this cell and the file can be found in \"Data\". As this is a huge dataset, we save the file as pickle then read it, as this is much less comutationally costly than directly working with the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Only intented to be run once\n",
    "saving_wikidata(DATA_PATH + \"all_wikidata.pck\")\"\"\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Loading and pre-processing of Quotebank data <a name=\"1.1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `clean_quotebank` (in [Functions](#2functions) below) does a first cleaning of the quotebank dataset by living behind quotes from unknown speaker as they would not be useful towards our study. We chose to also drop the quotes from uncertain speakers, ie: where the speaker probability is bellow 0.5. As this is a computationally costly operation, the cleaning is performed at the same time we will create our quotebank sub dataframes in the [Creation of the lists of similar professions](#professions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Loading wikidata labels <a name=\"1.2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wikidata labels are encoded in items called QIDs, to map them back to human readable labels, we upload the following dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "qid_label = pd.read_csv(DATA_PATH+'wikidata_labels_descriptions_quotebank.csv.bz2', compression='bz2', index_col='QID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a quick look at this data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Q31</th>\n",
       "      <td>Belgium</td>\n",
       "      <td>country in western Europe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q45</th>\n",
       "      <td>Portugal</td>\n",
       "      <td>country in southwestern Europe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q75</th>\n",
       "      <td>Internet</td>\n",
       "      <td>global system of connected computer networks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q148</th>\n",
       "      <td>People's Republic of China</td>\n",
       "      <td>sovereign state in East Asia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q155</th>\n",
       "      <td>Brazil</td>\n",
       "      <td>country in South America</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Label                                   Description\n",
       "QID                                                                           \n",
       "Q31                      Belgium                     country in western Europe\n",
       "Q45                     Portugal                country in southwestern Europe\n",
       "Q75                     Internet  global system of connected computer networks\n",
       "Q148  People's Republic of China                  sovereign state in East Asia\n",
       "Q155                      Brazil                      country in South America"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qid_label.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Pre-processing of wikidata <a name=\"1.3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to find information about the speakers, we use wikidata. Let's start by creating a data frame of all the speakers from our pickle file \"wikidata_all.pck\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikidata_all = pd.read_pickle(\"Data/wikidata_all.pck\")\n",
    "wikidata_all.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now search for possible redundant speakers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikidata_all['id'].is_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this returns `True`, we can see that there are no duplicate speakers in wikidata. We'll now create a new dataframe (`cleaned_wikidata`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of a clean version of the data (without modifying the original one)\n",
    "cleaned_wikidata = wikidata_all.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_wikidata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to remove rows where mandatory values for our study are missing like the name of the speaker and the gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_wikidata = cleaned_wikidata[~cleaned_wikidata.label.isin([None, \"None\"])]\n",
    "print(cleaned_wikidata.shape)\n",
    "cleaned_wikidata = cleaned_wikidata[~cleaned_wikidata.gender.isin([None, \"None\"])]\n",
    "print(cleaned_wikidata.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Exploratory Data Analysis of wikidata <a name=\"1.4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_wikidata['gender'] = cleaned_wikidata['gender'].apply(lambda x: x[0])\n",
    "print(\"There are {} different unique genders in wikidata\".format(cleaned_wikidata['gender'].unique().shape))\n",
    "cleaned_wikidata['gender'].unique()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's observe their distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_dist = cleaned_wikidata.groupby('gender').size()\n",
    "gender_dist = gender_dist.div(gender_dist.sum(axis=0), axis=0)\n",
    "pl = gender_dist.plot(kind='bar', figsize=[7,5])\n",
    "pl.set_title('Distribution of genders - log scale')\n",
    "pl.set_ylabel('participants')\n",
    "pl.set_xlabel('gender_dist')\n",
    "pl.set_yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qid_label.loc[cleaned_wikidata['gender'].unique()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_label = \"Q6581097\"\n",
    "female_label= \"Q6581072\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_dist[(gender_dist.index != male_label) & (gender_dist.index != female_label)].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that less than 0.03% of the data is neither considered a male or a female. For simplicity in our study we will ignore those cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_wikidata = cleaned_wikidata[cleaned_wikidata.gender.isin([male_label, female_label])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Only intented to be run once \n",
    "#Create occupation dataframe where everyone's occupation is known\n",
    "occupation = cleaned_wikidata[~cleaned_wikidata['occupation'].isin([None])]\n",
    "occupation.shape\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Only intented to be run once \n",
    "#Create complete datafram where occupation/religion/nationality/ethnic_group and academic_degree are known \n",
    "# for every speaker in the dataset\n",
    "wiki_clean = occupation[~(occupation['religion'].isin([None, \"None\"]) & occupation['nationality'].isin([None, \"None\"]) &\n",
    "                          occupation['ethnic_group'].isin([None, \"None\"]) & \n",
    "                          occupation['academic_degree'].isin([None, \"None\"]))]\n",
    "wiki_clean.shape \"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Only intented to be run once \n",
    "occupation.to_pickle(DATA_PATH + \"occupation.pck\")\n",
    "wiki_clean.to_pickle(DATA_PATH + \"wiki_clean.pck\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Only intented to be run once  \"\"\"\n",
    "wiki_occupation = pd.read_pickle(DATA_PATH + \"wiki_occupation.pck\")\n",
    "wiki_clean = pd.read_pickle(\"Data/wiki_clean.pck\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2.  Creation of the lists of similar professions <a name=\"professions\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that all the pre-processing is done, we can start our analysis. \n",
    "\n",
    "First, we will focus on the initial matter, being to what extent we can observe the differences in communicative acts in relation to gender within a professional area and whether there noticeable differences between those professional areas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions <a name=\"2functions\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_columns(column, target, init_df, name_column):\n",
    "    \"\"\"\n",
    "    Checks if a target ('politician', 'male', 'female' etc...) is in a certain column.\n",
    "    If it is, we return True in an additional column (name_column).\n",
    "    Inputs:\n",
    "        * column : name of column to search for target\n",
    "        * target : item of interest \n",
    "        * init_df : initial data frame\n",
    "        * name_column : name of new column of booleans\n",
    "    Outputs:\n",
    "        * final_df : dataframe with new column and only rows which contains the item\n",
    "    \"\"\"\n",
    "    final_df = init_df.copy(deep = True)\n",
    "    final_df[name_column] = final_df[column].apply(lambda x: np.any(x) in target)\n",
    "    return final_df\n",
    "\n",
    "\n",
    "def extracting_sub_df(quotebank, wikidata, column):\n",
    "    \"\"\"\n",
    "    Creates a sub dataframe with information from quotebank and wikidata.\n",
    "    We only take the rows in column which are True.\n",
    "    Inputs:\n",
    "        * quotebank : dataframe extracted from quotebank\n",
    "        * wikidata : dataframe extracted from wikidata\n",
    "        * column : column on which we base the merge\n",
    "    Outputs :\n",
    "        * sub_df : merged dataframe \n",
    "    \"\"\"\n",
    "    merged_df = pd.merge(quotebank, wikidata, left_on = 'speaker', right_on = 'label')\n",
    "    merged_df['qids'] = merged_df['qids'].apply(lambda x : x[0])\n",
    "    merged_df = merged_df[merged_df['qids'] == merged_df['id']]\n",
    "    sub_df = merged_df[merged_df[column] == True]\n",
    "    return sub_df\n",
    "\n",
    "\n",
    "def clean_quotebank(df):\n",
    "    \"\"\" \n",
    "    Cleans quotebank dataset by droping quotes from unknown speakers and\n",
    "    quotes where the speaker is uncertain (p<0.5)\n",
    "    Inputs:\n",
    "        * df : quotebank dataframe to clean    \n",
    "    \"\"\" \n",
    "    df_copy = df.copy(deep = True)\n",
    "    df_copy = df_copy[~df_copy.speaker.isin(['None', None])]\n",
    "    df_copy =  df_copy[df_copy['probas'].apply(lambda x: x[0][1]).values.astype(float) > 0.5]\n",
    "    return df_copy \n",
    "\n",
    "\n",
    "def create_df_with_conditions(column, condition, wikidata, column_temp, start, stop):\n",
    "    \"\"\"\n",
    "    From quotebank data, extracts and returns a data frame with only the rows that respect the condition.\n",
    "    Inputs:\n",
    "        * column : name of column where the condition is applied\n",
    "        * condition : condition of interest \n",
    "        * wikidata : wiki database\n",
    "        * column_temp : name of new column of booleans\n",
    "    Outputs:\n",
    "        * sub_df : dataframe with new column and only rows which contains the item\n",
    "    \"\"\"\n",
    "    wiki_plus = add_columns(column, condition, wikidata, column_temp)\n",
    "    sub_df = pd.DataFrame()\n",
    "    # Milestone 2: (20,21) and (15,16) = only for 2020 quotes; Milestone 3: (15,21) = quotes from 2015 to 2020\n",
    "    for i in range(start,stop):\n",
    "        DATA_FILE = 'quotes-20{}.json.bz2'.format(i)\n",
    "        with pd.read_json(DATA_PATH + DATA_FILE, lines = True, compression ='bz2', chunksize = 100000) as df_reader:\n",
    "            for chunk in df_reader:\n",
    "                sub_df = pd.concat([sub_df, extracting_sub_df(clean_quotebank(chunk), wiki_plus, column_temp)])\n",
    "    sub_df = sub_df.drop(column_temp, axis = 1)\n",
    "    return sub_df\n",
    "\n",
    "\n",
    "def merging(quotebank, wikidata):\n",
    "    \"\"\"\n",
    "    Creates a sub dataframe with information from quotebank and wikidata.\n",
    "    We only take the rows in column which are True.\n",
    "    Inputs:\n",
    "        * quotebank : dataframe extracted from quotebank\n",
    "        * wikidata : dataframe extracted from wikidata\n",
    "        * column : column on which we base the merge\n",
    "    Outputs :\n",
    "        * sub_df : merged dataframe \n",
    "    \"\"\"\n",
    "    merged_df = pd.merge(quotebank, wikidata, left_on = 'speaker', right_on = 'label')\n",
    "    merged_df['qids'] = merged_df['qids'].apply(lambda x : x[0])\n",
    "    merged_df = merged_df[merged_df['qids'] == merged_df['id']]\n",
    "    return merged_df\n",
    "\n",
    "\n",
    "def create_df(wikidata, start, stop):\n",
    "    \"\"\"\n",
    "    From quotebank data, extracts and returns a data frame with only the rows that respect the condition.\n",
    "    Inputs:\n",
    "        * column : name of column where the condition is applied\n",
    "        * condition : condition of interest \n",
    "        * wikidata : wiki database\n",
    "        * column_temp : name of new column of booleans\n",
    "    Outputs:\n",
    "        * sub_df : dataframe with new column and only rows which contains the item\n",
    "    \"\"\"\n",
    "    sub_df = pd.DataFrame()\n",
    "    # Milestone 2: (20,21) = only for 2020 quotes; Milestone 3: (15,21) = quotes from 2015 to 2020\n",
    "    for i in range(start, stop):\n",
    "        DATA_FILE = 'quotes-20{}.json.bz2'.format(i)\n",
    "        with pd.read_json(DATA_PATH + DATA_FILE, lines = True, compression ='bz2', chunksize = 100000) as df_reader:\n",
    "            for chunk in df_reader:\n",
    "                sub_df = pd.concat([sub_df, merging(clean_quotebank(chunk), wikidata)])\n",
    "    return sub_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Creation of professional fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create data frames with different fields of professions to compare quotations of men and women without the bias of the professional background. To do that, we manually selected various fields such as arts, science, economy and politics with different professions listed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "art_professions = qid_label[qid_label['Label'].str.contains('cineast|painter|musician|sculpter|architect|philosoph|writer|actor|dancer', na=False)]\n",
    "scientific_professions = qid_label[qid_label['Label'].str.contains('scientific|researcher|mathematician|doctor|astronomist|biologist|chemist|physicist|psychologist|sociologist|engineer|anatomist', na=False)]\n",
    "economic_professions = qid_label[qid_label['Label'].str.contains('economist|banke|financ|CEO|CTO|chairman|auditor|stockbroker|insurer|business manager|retail merchandizer|pricing analyst|statistician|financial consultant|salesperson|risk analyst|Data analyst|accountant|economic researcher|Investm|actuary', na=False)]\n",
    "political_professions = qid_label[qid_label['Label'].str.contains('politician|president|minister|government', na=False)]\n",
    "# Printing \n",
    "'''\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    print(economic_professions)\n",
    "''';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Creation of sub-dataframes per professions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to combine the information of the speakers from wikidata with our professional fields to create data frames of specific speakers' professions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we create data frames for year 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Only intented to be run once \n",
    "df_politicians_2020 = create_df_with_conditions('occupation', political_professions.index, wiki_occupation, 'ispolitician', 20, 21)\n",
    "df_artists_2020 = create_df_with_conditions('occupation', art_professions.index, wiki_occupation, 'isartist', 20, 21)\n",
    "df_scientists_2020 = create_df_with_conditions('occupation', scientific_professions.index, wiki_occupation, 'isscientist', 20, 21)\n",
    "df_economists_2020 = create_df_with_conditions('occupation', economic_professions.index, wiki_occupation, 'iseconomist', 20, 21)\"\"\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, we create data frames for year 2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Only intented to be run once \"\"\"\n",
    "df_politicians_2015 = create_df_with_conditions('occupation', political_professions.index, wiki_occupation, 'ispolitician', 15, 16)\n",
    "df_artists_2015 = create_df_with_conditions('occupation', art_professions.index, wiki_occupation, 'isartist', 15, 16)\n",
    "df_scientists_2015 = create_df_with_conditions('occupation', scientific_professions.index, wiki_occupation, 'isscientist', 15, 16)\n",
    "df_economists_2015 = create_df_with_conditions('occupation', economic_professions.index, wiki_occupation, 'iseconomist', 15, 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now generate pickle files where we store all the quotations of both genders with the same professional field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Only intented to be run once\n",
    "df_politicians_2020.to_pickle(DATA_PATH + \"politicians_2020.pck\")\n",
    "df_artists_2020.to_pickle(DATA_PATH + \"artists_2020.pck\")\n",
    "df_scientists_2020.to_pickle(DATA_PATH + \"scientists_2020.pck\")\n",
    "df_economists_2020.to_pickle(DATA_PATH + \"economists_2020.pck\")\"\"\";\n",
    "df_politicians_2015.to_pickle(DATA_PATH + \"politicians_2015.pck\")\n",
    "df_artists_2015.to_pickle(DATA_PATH + \"artists_2015.pck\")\n",
    "df_scientists_2015.to_pickle(DATA_PATH + \"scientists_2015.pck\")\n",
    "df_economists_2015.to_pickle(DATA_PATH + \"economists_2015.pck\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Saving of all sub data frames useful for processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we generated the pickle files, we can use them with the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_politicians_2020 = pd.read_pickle(DATA_PATH + \"politicians_2020.pck\")\n",
    "df_artists_2020 = pd.read_pickle(DATA_PATH + \"artists_2020.pck\")\n",
    "df_scientists_2020 = pd.read_pickle(DATA_PATH + \"scientists_2020.pck\")\n",
    "df_economists_2020 = pd.read_pickle(DATA_PATH + \"economists_2020.pck\")\n",
    "df_politicians_2015 = pd.read_pickle(DATA_PATH + \"politicians_2015.pck\")\n",
    "df_artists_2015 = pd.read_pickle(DATA_PATH + \"artists_2015.pck\")\n",
    "df_scientists_2015 = pd.read_pickle(DATA_PATH + \"scientists_2015.pck\")\n",
    "df_economists_2015 = pd.read_pickle(DATA_PATH + \"economists_2015.pck\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Creation of sub data frame without conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Only intented to be run once\n",
    "df_without_conditions_2020 = create_df(wiki_clean, 20, 21)\n",
    "df_without_conditions_2020.to_pickle(DATA_PATH + \"df_without_conditions_2020.pck\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_without_conditions_2020 = pd.read_pickle(DATA_PATH + \"df_without_conditions_2020.pck\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_df = df_without_conditions_2020.sample(n=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(small_df.shape)\n",
    "small_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We use textblob to identify quotations language\n",
    "from googletrans import Translator\n",
    "translator = Translator()\n",
    "small_df['quote_language'] = small_df['quotation'].apply(lambda x: translator.translate(x).src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_df['quote_language'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_dist = small_df.groupby('quote_language').size()\n",
    "language_dist = language_dist.div(language_dist.sum(axis=0), axis=0)\n",
    "pl = language_dist.plot(kind='bar', figsize=[7,5])\n",
    "pl.set_title('Distribution of languagefor the quotations')\n",
    "pl.set_ylabel('participants')\n",
    "pl.set_xlabel('language_dist')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Classification of the quotes <a name=\"classifier\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To distinguish between uncertain and certain quotations, we use the uncertainty detection classifier from the following paper \"P. A. Jean, S. Harispe, S. Ranwez, P. Bellot, and J. Montmain, “[Uncertainty detection in natural language: A probabilistic model](https://www.researchgate.net/publication/303842922)” ACM Int. Conf. Proceeding Ser., vol. 13-15-June, no. June, 2016, doi: 10.1145/2912845.2912873\". \n",
    "\n",
    "It's public git repository is: https://github.com/PAJEAN/uncertaintyDetection\n",
    "\n",
    "Still, as it was created 6 years ago, it only runs on python2. As we are using python3, we modified it to run for our project. We describe below how to run it.\n",
    "\n",
    "This uncertainty detection classifier is based on a very synthetic vector representation of sentences. Each dimension of this vector encrypts the main features of this sentence (e.g. uni-gram, bi-gram). The representation of the classifier is directly inspired by binary classification methods and returns only the uncertain quotations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pathways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As already mentionned in the [Pre-processing of the data](#pre-processing), the data folder \"txt_files\" must be downloaded and added in the same directory as this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PATH_TXT = 'txt_files/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quotes_to_txt(file_name, df):\n",
    "    \"\"\"\n",
    "    Changes quotes dataframe to a text file \n",
    "    Inputs:\n",
    "        * file_name : text name\n",
    "        * quotes : dataframe to convert\n",
    "    \"\"\"\n",
    "    quotes = df.quotation.astype(str)\n",
    "    with open(file_name, \"w\", encoding = \"utf-8\") as f:\n",
    "        for ind, quote in enumerate(quotes):\n",
    "            f.write(str(ind) + \" \" + quote + \"\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Creation of the text files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to run the classifier, we first need to create txt files with all the quotes and their index for each professional field. We already ran this cell and the files can be found in \"txt_files\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Only intented to be run once\n",
    "quotes_to_txt(PATH_TXT + \"politicians_2020.txt\", df_politicians_2020)\n",
    "quotes_to_txt(PATH_TXT + \"artists_2020.txt\", df_artists_2020)\n",
    "quotes_to_txt(PATH_TXT + \"scientists_2020.txt\", df_scientists_2020)\n",
    "quotes_to_txt(PATH_TXT + \"economists_2020.txt\", df_economists_2020)\n",
    "quotes_to_txt(PATH_TXT + \"df_without_conditions_2020.txt\", df_without_conditions_2020)\n",
    "\"\"\";\n",
    "quotes_to_txt(PATH_TXT + \"politicians_2015.txt\", df_politicians_2015)\n",
    "quotes_to_txt(PATH_TXT + \"artists_2015.txt\", df_artists_2015)\n",
    "quotes_to_txt(PATH_TXT + \"scientists_2015.txt\", df_scientists_2015)\n",
    "quotes_to_txt(PATH_TXT + \"economists_2015.txt\", df_economists_2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Use of the uncertainty detection classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating the files, we need to use the classifier to separate the uncertain and certain quotes. To continue, you must have downloaded the \"Classifier\" directory (see the [Pre-processing of the data](#pre-processing)) and add its files to the Classifier folder of the github repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **How to run the classifier**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to run the file MUD.py, you will need in your ADA environment: \n",
    "* Python 3.8\n",
    "* nltk library\n",
    "* numpy library\n",
    "* sklearn library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have downloaded and installed all the libraries in your envrionment, you're good to go.  \n",
    "\n",
    "To run the program, you have to open a terminal on jupyter notebook or an anaconda prompt on anaconda. You have to go in the project repository (in the main folder) and run the following command : **python MUD.py sfu Classifier/Input/name_of_the_file.txt**\n",
    "\n",
    "It is really important that you have the **ADA environment activated** to run this line.   \n",
    "\n",
    "The following files are the ones that you can put in the classifier (instead of name_of_the_file) :\n",
    "* politicians.txt\n",
    "* artists.txt\n",
    "* scientists.txt\n",
    "* economists.txt\n",
    "* df_without_conditions_2020.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **What does it returns ?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier will run for approximately 10 to 20 min depending on the file you give him. Once it is finished, you can find in the Data/Results folder two files, ***certainty_sentences*** and ***uncertainty_sentences***.  \n",
    "The file ***certainty_sentences*** : it is a txt file with all the quotes that have been considered as \"certain\" by the classifier; the file ***uncertainty_sentences*** is a txt file with all the quotes that have been considered as \"uncertain\" by the classifier.  \n",
    "\n",
    "You can find all these files under other names in function of the professions in the txt_files folder : \n",
    "* politicians_uncertainty.txt\n",
    "* artists_uncertainty.txt\n",
    "* scientists_uncertainty.txt\n",
    "* economists_uncertainty.txt\n",
    "* df_without_conditions_2020.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next section, we will load those files and start the statistical analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Statistical analysis <a name=\"analysis\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_lines(txt_file):\n",
    "    \"\"\"\n",
    "    Extracts the lines from a text file.\n",
    "    Inputs:\n",
    "        * txt_file : text file\n",
    "    Outputs:\n",
    "        * lines : all lines from text file\n",
    "    \"\"\"\n",
    "    lines = []\n",
    "    with open(txt_file, \"r\", encoding = \"utf8\") as file:\n",
    "        for line in file:\n",
    "            lines.append(line) \n",
    "    return lines\n",
    "\n",
    "\n",
    "def extract_indices(lines):\n",
    "    \"\"\"\n",
    "    Extracts the indices from every line.\n",
    "    Inputs:\n",
    "        * lines : all lines from text file\n",
    "    Outputs:\n",
    "        * indices : the indices from each lines\n",
    "    \"\"\"\n",
    "    indices = []\n",
    "    for i in range(0,len(lines)):\n",
    "        index = ''\n",
    "        line = lines[i]\n",
    "        for j in range(0,len(line)):\n",
    "            char = line[j]\n",
    "            if char.isspace():\n",
    "                break\n",
    "            else:\n",
    "                index += char\n",
    "        indices.append(index)\n",
    "    return indices\n",
    "\n",
    "\n",
    "def create_df_from_txt(txt_file, df_profession):\n",
    "    \"\"\"\n",
    "    Creates a sub data frame from df_profession containing the rows defined in the txt_file.\n",
    "    Inputs:\n",
    "        * txt_file : text file\n",
    "        * df_profession : data frame containing all the quotes from a professional field\n",
    "    Outputs:\n",
    "        * df_uncertain_profession : dataframe containing the uncertain quotes from this professional field\n",
    "    \"\"\"\n",
    "    df_uncertain_profession = df_profession.iloc[extract_indices(extract_lines(txt_file))]\n",
    "    return df_uncertain_profession\n",
    "\n",
    "\n",
    "def create_gender_dist(df_profession, df_uncertain_profession):\n",
    "    \"\"\"\n",
    "    Computes the gender distribution of uncertain speakers for a certain profession.\n",
    "    Inputs:\n",
    "        * df_profession : data frame containing all the quotes from a professional field\n",
    "        * df_uncertain_profession : dataframe containing the uncertain quotes from this professional field\n",
    "    Outputs:\n",
    "        * gender_dist : the gender distribution of uncertain speakers for a certain profession in percentage\n",
    "    \"\"\"\n",
    "    gender_dist = df_uncertain_profession.groupby('gender').size()/df_profession.groupby('gender').size() * 100\n",
    "    return gender_dist\n",
    "\n",
    "\n",
    "def plot_hist(df, profession):\n",
    "    ax = df.plot(kind='bar')\n",
    "    labels = qid_label.loc[df.index]['Label'].values\n",
    "    ax.set_ylabel('% of {}'.format(profession))\n",
    "    ax.set_xticklabels(labels)\n",
    "    plt.title('Relative percentage of uncertain {a}  within a gender'.format(a=profession,b=profession), pad=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4.1 Creation of the uncertain and certain dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2020\n",
    "df_uncertain_politicians_2020 = create_df_from_txt(PATH_TXT + \"politicians_uncertainty_2020.txt\", df_politicians_2020)\n",
    "df_uncertain_artists_2020 = create_df_from_txt(PATH_TXT + \"artists_uncertainty_2020.txt\", df_artists_2020)\n",
    "df_uncertain_scientists_2020 = create_df_from_txt(PATH_TXT + \"scientists_uncertainty_2020.txt\", df_scientists_2020)\n",
    "df_uncertain_economists_2020 = create_df_from_txt(PATH_TXT + \"economists_uncertainty_2020.txt\", df_economists_2020)\n",
    "#2015\n",
    "df_uncertain_politicians_2015 = create_df_from_txt(PATH_TXT + \"politicians_uncertainty_2015.txt\", df_politicians_2015)\n",
    "df_uncertain_artists_2015 = create_df_from_txt(PATH_TXT + \"artists_uncertainty_2015.txt\", df_artists_2015)\n",
    "df_uncertain_scientists_2015 = create_df_from_txt(PATH_TXT + \"scientists_uncertainty_2015.txt\", df_scientists_2015)\n",
    "df_uncertain_economists_2015 = create_df_from_txt(PATH_TXT + \"economists_uncertainty_2015.txt\", df_economists_2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2020\n",
    "df_certain_politicians_2020 = df_politicians_2020[~df_politicians_2020.quoteID.isin(df_uncertain_politicians_2020.quoteID)]\n",
    "df_certain_artists_2020 = df_artists_2020[~df_artists_2020.quoteID.isin(df_uncertain_artists_2020.quoteID)]\n",
    "df_certain_scientists_2020 = df_scientists_2020[~df_scientists_2020.quoteID.isin(df_uncertain_scientists_2020.quoteID)]\n",
    "df_certain_economists_2020 = df_economists_2020[~df_economists_2020.quoteID.isin(df_uncertain_economists_2020.quoteID)]\n",
    "#2015\n",
    "df_certain_politicians_2015 = df_politicians_2015[~df_politicians_2015.quoteID.isin(df_uncertain_politicians_2015.quoteID)]\n",
    "df_certain_artists_2015 = df_artists_2015[~df_artists_2015.quoteID.isin(df_uncertain_artists_2015.quoteID)]\n",
    "df_certain_scientists_2015 = df_scientists_2015[~df_scientists_2015.quoteID.isin(df_uncertain_scientists_2015.quoteID)]\n",
    "df_certain_economists_2015 = df_economists_2015[~df_economists_2015.quoteID.isin(df_uncertain_economists_2015.quoteID)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Analysis of the gender distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2020\n",
    "politicians_gender_dist_2020 = create_gender_dist(df_politicians_2020, df_uncertain_politicians_2020)\n",
    "economists_gender_dist_2020 = create_gender_dist(df_economists_2020, df_uncertain_economists_2020)\n",
    "artists_gender_dist_2020 = create_gender_dist(df_artists_2020, df_uncertain_artists_2020)\n",
    "scientists_gender_dist_2020 = create_gender_dist(df_scientists_2020, df_uncertain_scientists_2020)\n",
    "\n",
    "#2015\n",
    "politicians_gender_dist_2015 = create_gender_dist(df_politicians_2015, df_uncertain_politicians_2015)\n",
    "economists_gender_dist_2015 = create_gender_dist(df_economists_2015, df_uncertain_economists_2015)\n",
    "artists_gender_dist_2015 = create_gender_dist(df_artists_2015, df_uncertain_artists_2015)\n",
    "scientists_gender_dist_2015 = create_gender_dist(df_scientists_2015, df_uncertain_scientists_2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(17,4))\n",
    "\n",
    "plt.subplot(1,4,1)\n",
    "plot_hist(politicians_gender_dist_2020, 'politician')\n",
    "plt.subplot(1,4,2)\n",
    "plot_hist(artists_gender_dist_2020, 'artist')\n",
    "plt.subplot(1,4,3)\n",
    "plot_hist(scientists_gender_dist_2020, 'scientist')\n",
    "plt.subplot(1,4,4)\n",
    "plot_hist(economists_gender_dist_2020, 'economist')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4.3 Background influence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at our second question which investigates the roles culture, education and traditions play in determining those differences in speech between men and women."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first have a look at our features and which could relate to culture, education or tradition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that `nationality`, `ethnic_group` and `religion` could give cultural and traditional background information on the speaker.  \n",
    "While `academic_degree` can give information on the educational aspect of the quotation author."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Cultural background influence "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the function which allows to plot the labels of each cultural and educationnal category with respect to gender:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "influences = ['nationality', 'religion','ethnic_group', 'academic_degree']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_influence(df, name_influence):\n",
    "    df[name_influence] = qid_label.loc[df[name_influence]].Label.values\n",
    "    occur = df.groupby(name_influence)[name_influence].agg('count').sort_values(ascending=False).head(20)\n",
    "    df = df[df[name_influence].isin(occur.index)]\n",
    "    fem = df[df['gender'] == 'Q6581072']\n",
    "    mal = df[df['gender'] == 'Q6581097']\n",
    "    df = pd.concat([fem[name_influence].value_counts(normalize=True), mal[name_influence].value_counts(normalize=True)], axis=1, sort=True)\n",
    "    df.columns = [\"Women\", \"Men\"]\n",
    "    df.plot(kind='bar', figsize=(20,5))\n",
    "    plt.ylabel('Normalised count')\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.title('Number of uncertain quotations in function of {} and gender'.format(name_influence))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for influence in influences:\n",
    "    working_df = small_df.copy(deep=True)\n",
    "    working_df = working_df[~working_df[influence].isin([None, \"None\"])]\n",
    "    working_df[influence] = working_df[influence].apply(lambda x: x[0])\n",
    "    plot_influence(working_df, influence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a53c85f19ec6b2954a44487f2d44944431fd108845d569e7201686def1655526"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
