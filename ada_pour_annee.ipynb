{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADA - Project Milestone 2: \n",
    "# *Analysis of speech behaviours between genders*\n",
    "\n",
    "## Context\n",
    "\n",
    "In this project, we are going to analyze data from Quotebank. Quotebank, as the name suggests, is an open corpus which gathers 178 million quotations from 2008 to 2020. Still, in this project, we will only focus on the most recent quotations, being from 2015 to 2020.\n",
    "\n",
    "We are interested in using this dataset to answer the following question: Do speech behaviours related to confidence and uncertainty vary between men and women?\n",
    "\n",
    "To answer this question, we'll go through the following points:\n",
    "\n",
    "1. To what extent can we observe the differences in communicative acts in relation to gender within a professional area? Are there noticeable differences between those professional areas?\n",
    "3. What are the roles of environment (nationality), culture/tradition (religion, ethnic groups), and education (whether the speaker obtained an academic degree) in determining those differences in speech between men and women? How are the lines drawn between the language we use and the environment around us?\n",
    "4. Has there been a possible change over time (from 2015 to 2020)?\n",
    "\n",
    "In order to have access to the speakers' information (e.g., speakers' genders), we use the open source data from wikidata (https://www.wikidata.org/wiki/Wikidata:Main_Page).\n",
    "\n",
    "To analyse speech uncertainty, we use an uncertainty detection classifier, adapted from the following paper \"P. A. Jean, S. Harispe, S. Ranwez, P. Bellot, and J. Montmain, “[Uncertainty detection in natural language: A probabilistic model](https://www.researchgate.net/publication/303842922)” ACM Int. Conf. Proceeding Ser., vol. 13-15-June, no. June, 2016, doi: 10.1145/2912845.2912873\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Table of contents\n",
    "\n",
    "[1. Pre-processing of the data](#pre-processing) \n",
    "- [Imports](#1imports)\n",
    "- [Pathways](#1pathways)\n",
    "- [Functions](#1functions)\n",
    "- [Merging files from wikidata into one file containing the 9 million speakers](#1merging)\n",
    "- [1.1 Loading and pre-processing of Quotebank data](#1.1)\n",
    "- [1.2 Analysis of the quotes from Quotebank](#1.2)\n",
    "- [1.3 Loading wikidata labels](#1.3) \n",
    "- [1.4 Pre-processing of wikidata](#1.4)\n",
    "- [1.5 Exploratory Data Analysis of wikidata](#1.5)\n",
    "\n",
    "[2. Creation of our sub data frames](#dataframes)\n",
    "- [Functions](#2functions)\n",
    "- [2.1 Creation of professional fields](#2.1)\n",
    "- [2.2 Creation of sub-dataframes](#2.2) \n",
    "- [2.3 Saving of all sub data frames](#2.3)\n",
    "\n",
    "[3. Classification of the quotes](#classifier)\n",
    "- [Pathways](#3pathways)\n",
    "- [Functions](#3functions)\n",
    "- [3.1 Reading of all sub data frames](#3.1)\n",
    "- [3.2 Creation of the text files](#3.2)\n",
    "- [3.3 Use of the uncertainty detection classifier](#3.3) \n",
    "\n",
    "[4. Statistical-analysis](#stat_analysis)\n",
    "- [Functions](#4functions)\n",
    "- [4.1 Creation of the uncertain and certain dataframes](#4.1)\n",
    "- [4.2 Gender distribution accross occupations](#4.2)\n",
    "- [4.3 Analysis of the gender distribution per professions](#4.3) \n",
    "- [4.4 Background influence](#4.4)\n",
    "- [4.5 Possible variation from 2015 to 2020](#4.5)\n",
    "\n",
    "[5. First interpretation of results and upcoming steps](#interpretation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pre-processing of the data <a class = anchor id=\"pre-processing\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports <a class = anchor id=\"1imports\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by importing our libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_PATH contains all data frames\n",
    "DATA_PATH = 'Data/'\n",
    "# PATH_PARQUET contains all the data from wikidata\n",
    "PATH_PARQUET = 'Data_parquet/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saving_wikidata(path):\n",
    "    \"\"\"\n",
    "    Transforms all the wikidata files in one dataset saved as pickle. \n",
    "    This allows to load the file and use it more quickly and easily.\n",
    "    Inputs:\n",
    "        * path : pathway where to save the pickle file\n",
    "    \"\"\"\n",
    "    wikidata_all = pd.DataFrame()\n",
    "    for i in range(1,16):\n",
    "        if i < 10:\n",
    "            DATA_FILE = 'part-0000{}-0d587965-3d8f-41ce-9771-5b8c9024dce9-c000.snappy.parquet'.format(i)\n",
    "        else:\n",
    "            DATA_FILE = 'part-000{}-0d587965-3d8f-41ce-9771-5b8c9024dce9-c000.snappy.parquet'.format(i)\n",
    "        wikidata = pd.read_parquet(PATH_PARQUET + DATA_FILE)\n",
    "        wikidata_all = pd.concat([wikidata_all, wikidata])\n",
    "    wikidata_all.to_pickle(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "qid_label = pd.read_csv(DATA_PATH+'wikidata_labels_descriptions_quotebank.csv.bz2', compression='bz2', index_col='QID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_columns(column, target, init_df, name_column):\n",
    "    \"\"\"\n",
    "    Checks if a target ('politician', 'male', 'female' etc...) is in a certain column.\n",
    "    If it is, we return True in an additional column (name_column).\n",
    "    Inputs:\n",
    "        * column : name of column to search for target\n",
    "        * target : item of interest \n",
    "        * init_df : initial data frame\n",
    "        * name_column : name of new column of booleans\n",
    "    Outputs:\n",
    "        * final_df : dataframe with new column and only rows which contains the item\n",
    "    \"\"\"\n",
    "    final_df = init_df.copy(deep = True)\n",
    "    final_df[name_column] = final_df[column].apply(lambda x: np.any(x) in target)\n",
    "    return final_df\n",
    "\n",
    "\n",
    "def extracting_sub_df(quotebank, wikidata, column):\n",
    "    \"\"\"\n",
    "    Creates a sub dataframe with information from quotebank and wikidata.\n",
    "    We only take the rows in column which are True.\n",
    "    Inputs:\n",
    "        * quotebank : data frame extracted from quotebank\n",
    "        * wikidata : data frame extracted from wikidata\n",
    "        * column : column on which we base the merge\n",
    "    Outputs :\n",
    "        * sub_df : merged dataframe \n",
    "    \"\"\"\n",
    "    merged_df = pd.merge(quotebank, wikidata, left_on = 'speaker', right_on = 'label')\n",
    "    merged_df['qids'] = merged_df['qids'].apply(lambda x : x[0])\n",
    "    merged_df = merged_df[merged_df['qids'] == merged_df['id']]\n",
    "    sub_df = merged_df[merged_df[column] == True]\n",
    "    return sub_df\n",
    "\n",
    "\n",
    "def clean_quotebank(df):\n",
    "    \"\"\" \n",
    "    Cleans quotebank dataset by droping quotes from unknown speakers and\n",
    "    quotes where the speaker is uncertain (p<0.5)\n",
    "    Inputs:\n",
    "        * df : quotebank data frame to clean  \n",
    "    Outputs:\n",
    "        * df_copy : cleaned data frame\n",
    "    \"\"\" \n",
    "    df_copy = df.copy(deep = True)\n",
    "    df_copy = df_copy[~df_copy.speaker.isin(['None', None])]\n",
    "    df_copy =  df_copy[df_copy['probas'].apply(lambda x: x[0][1]).values.astype(float) > 0.5]\n",
    "    return df_copy \n",
    "\n",
    "\n",
    "def create_df_with_conditions(column, condition, wikidata, column_temp, start, stop):\n",
    "    \"\"\"\n",
    "    From quotebank data, extracts and returns a data frame with only the rows that respect the condition.\n",
    "    Inputs:\n",
    "        * column : name of column where the condition is applied\n",
    "        * condition : condition of interest \n",
    "        * wikidata : wiki database\n",
    "        * column_temp : name of new column of booleans\n",
    "    Outputs:\n",
    "        * sub_df : data frame with only rows which contains the item\n",
    "    \"\"\"\n",
    "    wiki_plus = add_columns(column, condition, wikidata, column_temp)\n",
    "    sub_df = pd.DataFrame()\n",
    "    # Milestone 2: (20,21) and (15,16) = only for 2020 quotes; Milestone 3: (15,21) = quotes from 2015 to 2020\n",
    "    for i in range(start,stop):\n",
    "        DATA_FILE = 'quotes-20{}.json.bz2'.format(i)\n",
    "        with pd.read_json(DATA_PATH + DATA_FILE, lines = True, compression ='bz2', chunksize = 100000) as df_reader:\n",
    "            for chunk in df_reader:\n",
    "                sub_df = pd.concat([sub_df, extracting_sub_df(clean_quotebank(chunk), wiki_plus, column_temp)])\n",
    "    sub_df = sub_df.drop(column_temp, axis = 1)\n",
    "    return sub_df\n",
    "\n",
    "\n",
    "def merging(quotebank, wikidata):\n",
    "    \"\"\"\n",
    "    Creates a sub dataframe with information from quotebank and wikidata.\n",
    "    Inputs:\n",
    "        * quotebank : data frame extracted from quotebank\n",
    "        * wikidata : data frame extracted from wikidata\n",
    "    Outputs :\n",
    "        * sub_df : merged data frame \n",
    "    \"\"\"\n",
    "    merged_df = pd.merge(quotebank, wikidata, left_on = 'speaker', right_on = 'label')\n",
    "    merged_df['qids'] = merged_df['qids'].apply(lambda x : x[0])\n",
    "    merged_df = merged_df[merged_df['qids'] == merged_df['id']]\n",
    "    return merged_df\n",
    "\n",
    "\n",
    "def create_df(wikidata, start, stop):\n",
    "    \"\"\"\n",
    "    From quotebank data, extracts and returns a data frame with only the rows where all informations are present.\n",
    "    Inputs:\n",
    "        * wikidata : wiki database\n",
    "        * start : years of beginning, for example 15 for 2015. Have to be between 15 and 21\n",
    "        * stop : years of end + 1, for example 16 if you only want 2015. Have to be between 15 and 21\n",
    "    Outputs:\n",
    "        * sub_df : data frame only the rows that have all the information of interest (without 'None')\n",
    "    \"\"\"\n",
    "    sub_df = pd.DataFrame()\n",
    "    # Milestone 2: (20,21) = only for 2020 quotes; Milestone 3: (15,21) = quotes from 2015 to 2020\n",
    "    for i in range(start, stop):\n",
    "        DATA_FILE = 'quotes-20{}.json.bz2'.format(i)\n",
    "        with pd.read_json(DATA_PATH + DATA_FILE, lines = True, compression ='bz2', chunksize = 100000) as df_reader:\n",
    "            for chunk in df_reader:\n",
    "                sub_df = pd.concat([sub_df, merging(clean_quotebank(chunk), wikidata)])\n",
    "    return sub_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "art_professions = qid_label[qid_label['Label'].str.contains('cineast|painter|musician|sculpter|architect|dancer| \\\n",
    "        philosoph|writer|actor|actress|choreographer|music interpreter|singer|photographer|entertainer', na=False)]\n",
    "\n",
    "scientific_professions = qid_label[qid_label['Label'].str.contains('scientific|researcher|mathematician|doctor| \\\n",
    "        astronomist|biologist|chemist|physicist|physician|psychologist|engineer|anatomist|neurologist| \\\n",
    "        pediatrician|veterinarian|pharmacist|obstetrician|gynecologist|therapist|dentist|surgeon|nurse| \\\n",
    "        psychiatrist|Scientific', na=False)]\n",
    "\n",
    "economic_professions = qid_label[qid_label['Label'].str.contains('economist|banker|financ|chairman|auditor|insurer| \\\n",
    "        CEO|chief executive officer|CTO|chief technology officer|CIO|chief investment officer|business manager| \\\n",
    "        stockbroker|retail merchandizer|pricing analyst|statistician|marketing consultant|sales consultant|actuary| \\\n",
    "        tax consultant| salesperson|risk analyst|data analyst|accountant|economic researcher|Investm|investor|', \\\n",
    "         na=False)]\n",
    "\n",
    "political_professions = qid_label[qid_label['Label'].str.contains('politician|president|minister|government \\\n",
    "        accountant General', na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Only intented to be run once\"\"\"\n",
    "wiki_occupation = pd.read_pickle(DATA_PATH + \"wiki_occupation.pck\")\n",
    "wiki_background = pd.read_pickle(DATA_PATH + \"wiki_background.pck\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, we create data frames for the year 2015. Again, as said above, the cell is already run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changer la data (deux arguments fonctions)\n",
    "\"\"\" Only intented to be run once \"\"\"\n",
    "df_politicians_2015 = create_df_with_conditions('occupation', political_professions.index, wiki_occupation, 'ispolitician', 15, 16)\n",
    "df_artists_2015 = create_df_with_conditions('occupation', art_professions.index, wiki_occupation, 'isartist', 15, 16)\n",
    "df_scientists_2015 = create_df_with_conditions('occupation', scientific_professions.index, wiki_occupation, 'isscientist', 15, 16)\n",
    "df_economists_2015 = create_df_with_conditions('occupation', economic_professions.index, wiki_occupation, 'iseconomist', 15, 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creation of a general background sub-dataframe (2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create our data frame from the more precise wikidata where the occupation, religion, nationality, ethnic_group and academic_degree are known for every speaker in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHanger data\n",
    "\"\"\" Only intented to be run once \"\"\"\n",
    "df_without_conditions_2015 = create_df(wiki_background, 15, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changer data\n",
    "\"\"\" Only intented to be run once \"\"\"\n",
    "#2015\n",
    "df_politicians_2015.to_pickle(DATA_PATH + \"politicians_2015.pck\")\n",
    "df_artists_2015.to_pickle(DATA_PATH + \"artists_2015.pck\")\n",
    "df_scientists_2015.to_pickle(DATA_PATH + \"scientists_2015.pck\")\n",
    "df_economists_2015.to_pickle(DATA_PATH + \"economists_2015.pck\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changer date\n",
    "\"\"\" Only intented to be run once \"\"\"\n",
    "df_without_conditions_2015.to_pickle(DATA_PATH + \"df_without_conditions_2015.pck\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # ICI Restart le kernel pour libérer espace (refaire import et fonction save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changer année\n",
    "df_politicians_2015 = pd.read_pickle(DATA_PATH + \"politicians_2015.pck\")\n",
    "df_artists_2015 = pd.read_pickle(DATA_PATH + \"artists_2015.pck\")\n",
    "df_scientists_2015 = pd.read_pickle(DATA_PATH + \"scientists_2015.pck\")\n",
    "df_economists_2015 = pd.read_pickle(DATA_PATH + \"economists_2015.pck\")\n",
    "df_without_conditions_2015 = pd.read_pickle(DATA_PATH + \"df_without_conditions_2015.pck\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_politicians_2015['quote_language'] = df_politicians_2015['quotation'].apply(lambda x: detect(x) if len(x) > 50 else 'not_a_language')\n",
    "df_politicians_2015 = df_politicians_2015[df_politicians_2015['quote_language'] == 'en']\n",
    "\n",
    "df_artists_2015['quote_language'] = df_artists_2015['quotation'].apply(lambda x: detect(x) if len(x) > 50 else 'not_a_language')\n",
    "df_artists_2015 = df_artists_2015[df_artists_2015['quote_language'] == 'en']\n",
    "\n",
    "df_scientists_2015['quote_language'] = df_scientists_2015['quotation'].apply(lambda x: detect(x) if len(x) > 50 else 'not_a_language')\n",
    "df_scientists_2015 = df_scientists_2015[df_scientists_2015['quote_language'] == 'en']\n",
    "\n",
    "df_economists_2015['quote_language'] = df_economists_2015['quotation'].apply(lambda x: detect(x) if len(x) > 50 else 'not_a_language')\n",
    "df_economists_2015 = df_economists_2015[df_economists_2015['quote_language'] == 'en']\n",
    "\n",
    "df_without_conditions_2015['quote_language'] = df_without_conditions_2015['quotation'].apply(lambda x: detect(x) if len(x) > 50 else 'not_a_language')\n",
    "df_without_conditions_2015 = df_without_conditions_2015[df_without_conditions_2015['quote_language'] == 'en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changer data\n",
    "\"\"\" Only intented to be run once \"\"\"\n",
    "#2015\n",
    "df_politicians_2015.to_pickle(DATA_PATH + \"politicians_2015.pck\")\n",
    "df_artists_2015.to_pickle(DATA_PATH + \"artists_2015.pck\")\n",
    "df_scientists_2015.to_pickle(DATA_PATH + \"scientists_2015.pck\")\n",
    "df_economists_2015.to_pickle(DATA_PATH + \"economists_2015.pck\")\n",
    "df_without_conditions_2015.to_pickle(DATA_PATH + \"df_without_conditions_2015.pck\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PATH_TXT = 'txt_files/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quotes_to_txt(file_name, df):\n",
    "    \"\"\"\n",
    "    Changes quotes dataframe to a text file \n",
    "    Inputs:\n",
    "        * file_name : text name\n",
    "        * df : dataframe to convert\n",
    "    \"\"\"\n",
    "    quotes = df.quotation.astype(str)\n",
    "    with open(file_name, \"w\", encoding = \"utf-8\") as f:\n",
    "        for ind, quote in enumerate(quotes):\n",
    "            f.write(str(ind) + \" \" + quote + \"\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Creation of the text files <a class = anchor id=\"3.2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changer date\n",
    "\"\"\" Only intented to be run once \"\"\"\n",
    "quotes_to_txt(PATH_TXT + \"politicians_2015.txt\", df_politicians_2015)\n",
    "quotes_to_txt(PATH_TXT + \"artists_2015.txt\", df_artists_2015)\n",
    "quotes_to_txt(PATH_TXT + \"scientists_2015.txt\", df_scientists_2015)\n",
    "quotes_to_txt(PATH_TXT + \"economists_2015.txt\", df_economists_2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sub data frame without conditions on professions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changer date\n",
    "\"\"\" Only intented to be run once\"\"\"\n",
    "quotes_to_txt(PATH_TXT + \"df_without_conditions_2015.txt\", df_without_conditions_2015)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a53c85f19ec6b2954a44487f2d44944431fd108845d569e7201686def1655526"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
